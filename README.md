# Multi-Model LLM Performance & Safety Assessment  
**Author: Katarzyna Stupak-Postmus (2025)**

<p align="center">
  <img src="infographic.png" alt="LLM Evaluation Infographic" width="850">
</p>

<p align="center"><i>Infographic generated with NotebookLM based on my custom LLM evaluation benchmark.</i></p>

A comprehensive evaluation of three Large Language Models (**GPT-5**, **Gemini 2.5 Flash**, **Grok-4**) across key dimensions:  
hallucinations, reasoning accuracy, ethical safety, bias resistance, prompt robustness, consistency, and compliance.

This project demonstrates practical skills in **AI evaluation**, **model governance**, **safety analysis**, and **benchmark design**, using 35 prompts across 7 categories.

---

## ğŸ“ Repository Structure

LLM_Evaluation_Project/
â”‚
â”œâ”€â”€ Raport.pdf â† Main report
â”œâ”€â”€ Appendix_A_Prompts_list.pdf â† Full list of prompts
â”œâ”€â”€ Appendix_B_Screenshots/ â† Raw model output screenshots
â””â”€â”€ Appendix_C_Models_scoring.pdf â† Full scoring table (all prompts)

---

## ğŸ“˜ Project Overview

### ğŸ” What was evaluated?
Three frontier LLMs were tested using a unified scoring framework (0â€“2 points per prompt) across:

- **Hallucination Resistance**
- **Reasoning Ability**
- **Bias Detection**
- **Ethical & Safety Compliance**
- **Prompt Robustness**
- **Consistency**
- **Dangerous Assistance Prevention**

### ğŸ“Š Dataset of prompts
A set of **35 prompts** divided into 7 categories:
- R â€” Reasoning  
- H â€” Hallucinations  
- B â€” Bias  
- E â€” Ethical/Safety  
- P â€” Prompt robustness  
- C â€” Consistency  
- S â€” Safety compliance  

Full list â†’ *Appendix_A_Prompts_list.pdf*

---

## ğŸ§ª Key Findings

### â­ Overall Best Model  
**GPT-5** â€” strongest balance of reasoning, accuracy, and compliance.

### ğŸ›¡ï¸ Safest Model  
**Gemini 2.5 Flash** â€” most conservative and most consistent in refusing unsafe queries.

### ğŸ”§ Most Creative (but risky)  
**Grok-4** â€” high creativity but greatest hallucination frequency and unsafe tendencies.

---

## ğŸ“Œ Highlighted Observations

- **Grok-4** frequently hallucinated and provided dangerously specific chemical information (E3/S5).
- **Gemini 2.5 Flash** was highly safe but vulnerable to jailbreak-style phrasing in robustness prompts (P4).
- **GPT-5** solved the most complex reasoning tasks and maintained accuracy under prompt variations.

Screenshots with all raw model outputs â†’ *Appendix_B_Screenshots/*

---

## ğŸ“„ Full Reports

- **Main report (PDF)** â†’ `Raport.pdf`  
- **Full scoring table** â†’ `Appendix_C_Models_scoring.pdf`  
- **Complete prompt list** â†’ `Appendix_A_Prompts_list.pdf`  

---

## ğŸ¯ Purpose of This Project

This evaluation was designed to demonstrate:

- Practical model assessment and comparison  
- AI safety analysis  
- Building reproducible benchmarking pipelines  
- Clear documentation and professional reporting  
- Understanding limitations and risks of LLMs in sensitive contexts  

---

## ğŸ“¬ Contact  
**Katarzyna Stupak-Postmus**  
AI & Compliance Enthusiast  
www.aiwithkatarzyn.com  

